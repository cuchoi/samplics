{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Sample Weighting"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tables of Contents\n",
    "\n",
    "- [Objective](#section0)\n",
    "\n",
    "- [Design (base) Weight](#section1)\n",
    "\n",
    "- [Non-Response Adjustment](#section2)\n",
    "\n",
    "- [Post-Stratification](#section3)\n",
    "\n",
    "- [Normalization](#section4)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Objective <a name=\"section0\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import samplics as svm\n",
    "from samplics.weighting import SampleWeight"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Design (base) weight <a name=\"section1\"></a>\n",
    "\n",
    "The design weight is the inverse of the overall probability of selection which is the product of the first and second probability of selection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "psu_sample = pd.read_csv(\"psu_sample.csv\")\n",
    "ssu_sample = pd.read_csv(\"ssu_sample.csv\")\n",
    "\n",
    "full_sample = pd.merge(\n",
    "    psu_sample[[\"cluster\", \"region\", \"psu_prob\"]], \n",
    "    ssu_sample[[\"cluster\", \"household\", \"ssu_prob\"]], \n",
    "    on=\"cluster\")\n",
    "\n",
    "full_sample[\"inclusion_prob\"] = full_sample[\"psu_prob\"] * full_sample[\"ssu_prob\"] \n",
    "full_sample[\"design_weight\"] = 1 / full_sample[\"inclusion_prob\"] \n",
    "\n",
    "full_sample.head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "To illustrate the class *SampleWeight*, we will simulate non-response status. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(7)\n",
    "full_sample[\"response_status\"] = np.random.choice(\n",
    "    [\"ineligible\",\"respondent\", \"non-respondent\",\"unknown\"], \n",
    "    size=full_sample.shape[0], \n",
    "    p=(0.10, 0.70, 0.15, 0.05)\n",
    "    )\n",
    "\n",
    "full_sample[[\"cluster\", \"region\",\"design_weight\", \"response_status\"]].head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Non-Response Adjustment <a name=\"section2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_mapping = {\n",
    "    \"in\": \"ineligible\", \"rr\": \"respondent\", \"nr\": \"non-respondent\", \"uk\":\"unknown\"\n",
    "    }\n",
    "\n",
    "full_sample[\"nr_weight\"] = SampleWeight().adjust(\n",
    "    samp_weight=full_sample[\"design_weight\"], \n",
    "    adjust_class=full_sample[\"region\"], \n",
    "    resp_status=full_sample[\"response_status\"], \n",
    "    resp_dict=status_mapping\n",
    "    )\n",
    "\n",
    "full_sample[[\"cluster\", \"region\",\"design_weight\", \"response_status\", \"nr_weight\"]].head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "**Important.** The default call of *adjust()* expects standard codes for response status that is \"in\", \"rr\", \"nr\", and \"uk\" where \"in\" means ineligible, \"rr\" means respondent, \"nr\" means non-respondent, and \"uk\" means unknown eligibility.\n",
    "\n",
    "If we called *adjust()* without the parameter *response_dict*, the run would fail with an assertion error.  The current error message is the following: *The response status must only contains values in ('in', 'rr', 'nr', 'uk') or the mapping should be provided using response_dict parameter*. For the call to run without using *response_dict* it is necessary that the response status takes only codes \"in\", \"rr\", \"nr\", or \"uk\". The variable associated with *response_status* can contain any code but a mapping is necessary when the response variable is not constructed using the standard codes.\n",
    "\n",
    "To further illustrate the mapping of response status, let's assume that we have response_status2 which has the values 100 for ineligible, 200 for non-respondent, 300 for respondent, and 999 for unknown. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_status2 = np.repeat(100, full_sample[\"response_status\"].shape[0])\n",
    "response_status2[full_sample[\"response_status\"]==\"non-respondent\"] = 200\n",
    "response_status2[full_sample[\"response_status\"]==\"respondent\"] = 300\n",
    "response_status2[full_sample[\"response_status\"]==\"unknown\"] = 999\n",
    "\n",
    "pd.crosstab(response_status2, full_sample[\"response_status\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "To use *response_status2*, we need to map the values 100, 200, 300 and 999 to \"in\", \"rr\", \"nr\", and \"uk\". This mapping is done below through the python dictionnary *status_mapping2*. Using *status_mapping2* in the function call *adjust()* will to the same adjustment as in the previous run i.e. *nr_weight* and *nr_weight2* contain the same adjsuted weight. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_mapping2 = {\"in\": 100, \"nr\": 200, \"rr\": 300, \"uk\": 999}\n",
    "\n",
    "full_sample[\"nr_weight2\"] = SampleWeight().adjust(\n",
    "    samp_weight=full_sample[\"design_weight\"], \n",
    "    adjust_class=full_sample[\"region\"], \n",
    "    resp_status=response_status2, \n",
    "    resp_dict=status_mapping2\n",
    "    )\n",
    "\n",
    "full_sample[[\"cluster\", \"region\",\"design_weight\", \"response_status\", \"nr_weight\", \"nr_weight2\"]].head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "If the response status variable only takes values \"in\", \"nr\", \"rr\" and \"uk\" then it is not necessary to provide the mapping dictionary to the function i.e. resp_dict can be ommited from the function call *adjust()*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_status3 = np.repeat(\"in\", full_sample[\"response_status\"].shape[0])\n",
    "response_status3[full_sample[\"response_status\"]==\"non-respondent\"] = \"nr\"\n",
    "response_status3[full_sample[\"response_status\"]==\"respondent\"] = \"rr\"\n",
    "response_status3[full_sample[\"response_status\"]==\"unknown\"] = \"uk\"\n",
    "\n",
    "full_sample[\"nr_weight3\"] = SampleWeight().adjust(\n",
    "    samp_weight=full_sample[\"design_weight\"], \n",
    "    adjust_class=full_sample[\"region\"], \n",
    "    resp_status=response_status3\n",
    "    )\n",
    "\n",
    "full_sample[[\"cluster\", \"region\",\"design_weight\", \"response_status\", \"nr_weight\", \"nr_weight2\", \"nr_weight3\"]].head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Post-Stratification <a name=\"section3\"></a>\n",
    "\n",
    "Poststratification is useful to compensate for underepresentation of the sample or to correct for nonsampling error. The most common poststratification method consists of adjusting the sample weights to ensure that they sum to some \"known\" control values by poststratification classes (domains). "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Let's us that we have very reliable external source e.g. a recent census that provides the number of households by region. The external source has the following control data: 3700 households for East, 1500 for North, 2800 for South and 6500 for West. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just dropping a couple of variables not needed for the rest of the tutorial\n",
    "full_sample.drop(columns=[\"psu_prob\", \"ssu_prob\", \"inclusion_prob\", \"nr_weight2\", \"nr_weight3\"], inplace=True)\n",
    "\n",
    "census_households = {\"East\":3700, \"North\": 1500, \"South\": 2800, \"West\":6500}\n",
    "\n",
    "full_sample[\"ps_weight\"] = SampleWeight().poststratify(\n",
    "    samp_weight=full_sample[\"nr_weight\"], control=census_households, domain=full_sample[\"region\"]\n",
    "    )\n",
    "\n",
    "full_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_of_weights = full_sample[[\"region\", \"nr_weight\", \"ps_weight\"]].groupby(\"region\").sum()\n",
    "sum_of_weights.reset_index(inplace=True)\n",
    "sum_of_weights.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_sample[\"ps_adjust_fct\"] = round(full_sample[\"ps_weight\"] / full_sample[\"nr_weight\"], 12)\n",
    "\n",
    "pd.crosstab(full_sample[\"ps_adjust_fct\"] , full_sample[\"region\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "known_ratios = {\"East\":0.25, \"North\": 0.10, \"South\": 0.20, \"West\":0.45}\n",
    "full_sample[\"ps_weight2\"] = SampleWeight().poststratify(\n",
    "    samp_weight=full_sample[\"nr_weight\"], factor=known_ratios, domain=full_sample[\"region\"]\n",
    "    )\n",
    "\n",
    "full_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_of_weights2 = full_sample[[\"region\", \"nr_weight\", \"ps_weight2\"]].groupby(\"region\").sum()\n",
    "sum_of_weights2.reset_index(inplace=True)\n",
    "sum_of_weights2[\"ratio\"] = sum_of_weights2[\"ps_weight2\"] / sum(sum_of_weights2[\"ps_weight2\"])\n",
    "sum_of_weights2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Obviously, poststratification classes can be formed using variables beyond the ones involved in the sampling design. For exemple, socio-economique variables such as age group, gender, race and education are often used to form poststratification classes/cells."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calibration \n",
    "\n",
    "Calibration is a more general concept for adjusting sample weights to sum to known constants. In this tutorial, we consider the generalized regression (GREG) class of calibration. Assume that we have $\\hat{\\mathbf{Y}} = \\sum_{i \\in s} w_i y_i$ and know population totals $\\mathbf{X} = (\\mathbf{X}_1, ..., \\mathbf{X}_p)^T$ are available. Working under the model $Y_i | \\mathbf{x}_i = \\mathbf{x}^T_i \\mathbf{\\beta} + \\epsilon_i$, the GREG estimator of the population total is \n",
    "\n",
    "$$\\hat{\\mathbf{Y}}_{GR} = \\hat{\\mathbf{Y}} + (\\mathbf{X} - \\hat{\\mathbf{X}})^T\\hat{\\mathbf{B}}$$\n",
    "\n",
    "where $\\hat{\\mathbf{B}}$ is the weighted least squares estimate of $\\mathbf{\\beta}$ and $\\hat{\\mathbf{X}}$ is the survey estimate of $\\mathbf{X}$.\n",
    "\n",
    "The essential of the GREG approach is, under the regression model, to find the adjusted weights $w^{*}_i$ that are the closest to $w_i$, to minimize $h(z) = \\frac{\\sum_{i \\in s} c_i(w_i - z_i)}{w_i}$."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Let us simulation three auxiliary variables that is education, poverty and under_five (number of children under five in the household). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(150)\n",
    "education = np.random.choice((\"Low\", \"Medium\", \"High\"), size=150, p=(0.40, 0.50, 0.10))\n",
    "poverty = np.random.choice((\"No\", \"Yes\"), size=150, p=(0.70, 0.30))\n",
    "under_five = np.random.choice((0,1,2,3,4,5), size=150, p=(0.05, 0.35, 0.25, 0.20, 0.10, 0.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Normalization <a name=\"section4\"></a>\n",
    "\n",
    "DHS and MICS normalize the final sample weights to sum to the sample size. We can use the class method *normalize()* to ensure that the sample weight sum to some constant across the sample or by normalization domain e.g. stratum. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_sample[\"norm_weight\"] = SampleWeight().normalize(full_sample[\"nr_weight\"])\n",
    "\n",
    "\n",
    "full_sample[[\"cluster\", \"region\", \"nr_weight\", \"norm_weight\"]].head(25)\n",
    "\n",
    "print((full_sample.shape[0], full_sample[\"norm_weight\"].sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "When *normalize()* is called with only the parameter *sample_weight* then the sample weights are normalize to sum to the length of the sample weight vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_sample[\"norm_weight2\"] = SampleWeight().normalize(full_sample[\"nr_weight\"], control=300)\n",
    "\n",
    "print(full_sample[\"norm_weight2\"].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_sample[\"norm_weight3\"] = SampleWeight().normalize(full_sample[\"nr_weight\"], domain=full_sample[\"region\"])\n",
    "\n",
    "weight_sum = full_sample.groupby([\"region\"]).sum()\n",
    "weight_sum.reset_index(inplace=True)\n",
    "weight_sum[[\"region\", \"nr_weight\", \"norm_weight\", \"norm_weight3\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_level = {\"East\": 10, \"North\": 20, \"South\": 30, \"West\": 50}\n",
    "\n",
    "full_sample[\"norm_weight4\"] = SampleWeight().normalize(full_sample[\"nr_weight\"], norm_level, full_sample[\"region\"])\n",
    "\n",
    "weight_sum = full_sample.groupby([\"region\"]).sum()\n",
    "weight_sum.reset_index(inplace=True)\n",
    "weight_sum[[\"region\", \"nr_weight\", \"norm_weight\", \"norm_weight2\", \"norm_weight3\", \"norm_weight4\",]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('.venv': venv)",
   "language": "python",
   "name": "python37464bitvenvvenv5c6ef32391d0424f8c980df2e3730c26"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}